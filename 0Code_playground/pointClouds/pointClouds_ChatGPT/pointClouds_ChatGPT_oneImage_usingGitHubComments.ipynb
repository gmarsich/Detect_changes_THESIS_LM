{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment: thesisPlayground_pointClouds_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the files with the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5_file(file_path, dataset_key):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = np.array(f[dataset_key])\n",
    "    return data\n",
    "\n",
    "def load_transformation_matrix_by_name(csv_file, row_name):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    matrix_row = df[df.iloc[:, 0] == row_name].iloc[0]\n",
    "    M = matrix_row[['M_cam_from_uv_00', 'M_cam_from_uv_01', 'M_cam_from_uv_02',\n",
    "                    'M_cam_from_uv_10', 'M_cam_from_uv_11', 'M_cam_from_uv_12',\n",
    "                    'M_cam_from_uv_20', 'M_cam_from_uv_21', 'M_cam_from_uv_22']].values\n",
    "    return M.reshape((3, 3))\n",
    "\n",
    "def create_point_cloud_from_image_and_position(image_data, position_data, transformation_matrix=None, scale_factor=1.0):\n",
    "    # Ensure both data arrays have the same shape\n",
    "    assert image_data.shape[:2] == position_data.shape[:2], \"Image and position data must have the same shape\"\n",
    "\n",
    "    # Flatten the image and position data\n",
    "    height, width = image_data.shape[:2]\n",
    "    colors = image_data.reshape(-1, image_data.shape[2]) if image_data.ndim == 3 else image_data.reshape(-1, 1)\n",
    "    positions = position_data.reshape(-1, position_data.shape[2])\n",
    "\n",
    "    # Apply scale factor to positions\n",
    "    positions *= scale_factor\n",
    "\n",
    "    # Apply transformation matrix if provided\n",
    "    if transformation_matrix is not None:\n",
    "        positions = positions @ transformation_matrix.T\n",
    "\n",
    "    # Create an Open3D PointCloud object\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    \n",
    "    # Set the points\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(positions)\n",
    "    \n",
    "    # Normalize the colors to [0, 1] if they are not already in this range\n",
    "    if colors.max() > 1.0:\n",
    "        colors = colors / 255.0\n",
    "\n",
    "    # Set the colors\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "def generate_point_cloud(images, positions, csv_file, row_name, image_dataset_key='data', position_dataset_key='data', scale_factor=1.0):\n",
    "    # Load transformation matrix from the specified row in the CSV\n",
    "    transformation_matrix = load_transformation_matrix_by_name(csv_file, row_name)\n",
    "\n",
    "    point_clouds = []\n",
    "\n",
    "    for image_file, position_file in zip(images, positions):\n",
    "        # Load image and position data from the HDF5 files\n",
    "        image_data = load_hdf5_file(image_file, 'dataset')\n",
    "        position_data = load_hdf5_file(position_file, 'dataset')\n",
    "\n",
    "        # Create point cloud from image and position data\n",
    "        point_cloud = create_point_cloud_from_image_and_position(image_data, position_data, transformation_matrix, scale_factor)\n",
    "        \n",
    "        # Add the point cloud to the list\n",
    "        point_clouds.append(point_cloud)\n",
    "    \n",
    "    return point_clouds\n",
    "    # Load transformation matrix from the specified row in the CSV\n",
    "    transformation_matrix = load_transformation_matrix_by_name(csv_file, row_name)\n",
    "\n",
    "    point_clouds = []\n",
    "\n",
    "    for image_file, position_file in zip(images, positions):\n",
    "        # Load image and position data from the HDF5 files\n",
    "        image_data = load_hdf5_file(image_file, image_dataset_key)\n",
    "        position_data = load_hdf5_file(position_file, position_dataset_key)\n",
    "\n",
    "        # Create point cloud from image and position data\n",
    "        point_cloud = create_point_cloud_from_image_and_position(image_data, position_data, transformation_matrix, scale_factor)\n",
    "        \n",
    "        # Add the point cloud to the list\n",
    "        point_clouds.append(point_cloud)\n",
    "    \n",
    "    return point_clouds\n",
    "\n",
    "    point_clouds = []\n",
    "\n",
    "\n",
    "def render_point_cloud(point_cloud):\n",
    "\n",
    "    pcd = point_cloud\n",
    "\n",
    "    # Create a visualization window\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "\n",
    "    # Add the point cloud to the visualization window\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    # Set the render options (optional)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.point_size = 2  # Adjust the size of the points\n",
    "\n",
    "    # Render the visualization\n",
    "    vis.run()\n",
    "\n",
    "    # Close the visualization window\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = '/local/home/gmarsich/Desktop/Thesis/0Code_playground/pointClouds/pointClouds_ChatGPT/metadata_camera_parameters.csv'\n",
    "base_path = '/local/home/gmarsich/data2TB/Hypersim/evermotion_dataset/scenes'\n",
    "scene = 'ai_007_008'  # name of the scene, with format ai_VVV_NNN\n",
    "cam_xx = 'cam_00'\n",
    "\n",
    "# Get list of image and position HDF5 files\n",
    "image_files = sorted(glob.glob(os.path.join(base_path, scene, 'images', 'scene_' + cam_xx + '_final_hdf5', '*.color.hdf5')))\n",
    "position_files = sorted(glob.glob(os.path.join(base_path, scene, 'images', 'scene_' + cam_xx + '_geometry_hdf5', '*.position.hdf5')))\n",
    "\n",
    "# Ensure the number of image and position files match\n",
    "if len(image_files) != len(position_files):\n",
    "    raise ValueError(\"The number of image files and position files do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of point clouds\n",
    "point_clouds = generate_point_cloud(image_files, position_files, csv_file = path_metadata, row_name=scene, scale_factor=1)\n",
    "\n",
    "# Visualize the first point cloud as an example\n",
    "render_point_cloud(point_clouds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using images, depths and intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrinsics_from_metadata_COMPLETE(scene_row):\n",
    "    \"\"\"\n",
    "    Derive the intrinsic matrix from metadata.\n",
    "\n",
    "    Parameters:\n",
    "        scene_row (pd.Series): Metadata row for the scene.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The intrinsic matrix K.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the necessary parameters from the metadata\n",
    "    img_width = scene_row[\"settings_output_img_width\"]\n",
    "    img_height = scene_row[\"settings_output_img_height\"]\n",
    "    focal_length = scene_row[\"camera_physical_focal_length\"]\n",
    "    sensor_width = scene_row[\"camera_physical_film_width\"]\n",
    "\n",
    "    # Calculate fx and fy using the focal length and sensor width\n",
    "    fx = focal_length / sensor_width * img_width\n",
    "    fy = fx  # Assuming square pixels\n",
    "\n",
    "    # Calculate the principal point (cx, cy) considering horizontal and vertical shifts\n",
    "    cx = img_width / 2 + scene_row[\"camera_physical_horizontal_shift\"]\n",
    "    print('img_width: ', img_width)\n",
    "    print('camera_physical_horizontal_shift\":', scene_row[\"camera_physical_horizontal_shift\"])\n",
    "    cy = img_height / 2 + scene_row[\"camera_physical_lens_shift\"]\n",
    "    print('img_height : ', img_height )\n",
    "    print('camera_physical_lens_shift\":', scene_row[\"camera_physical_lens_shift\"])\n",
    "\n",
    "    # Construct the intrinsic matrix\n",
    "    intrinsic_matrix = np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    return intrinsic_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_intrinsics_with_tilt_shift_COMPLETE(path_metadata, scene_name):\n",
    "    \"\"\"\n",
    "    Load camera metadata from a CSV file and compute the intrinsic matrix considering tilt-shift parameters.\n",
    "\n",
    "    Parameters:\n",
    "        path_metadata (str): Path to the metadata CSV file.\n",
    "        scene_name (str): Name of the scene to analyze.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated intrinsic matrix K.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the metadata CSV file\n",
    "    df = pd.read_csv(path_metadata)\n",
    "    \n",
    "    # Filter the DataFrame to get the row corresponding to the specified scene\n",
    "    scene_row = df[df['scene_name'] == scene_name]\n",
    "    \n",
    "    if scene_row.empty:\n",
    "        raise ValueError(f\"Scene '{scene_name}' not found in the metadata.\")\n",
    "    \n",
    "    # Use the first matching row\n",
    "    scene_row = scene_row.iloc[0]\n",
    "    \n",
    "    # Get the intrinsic matrix from metadata\n",
    "    intrinsic_matrix = get_intrinsics_from_metadata_COMPLETE(scene_row)\n",
    "    \n",
    "    return intrinsic_matrix\n",
    "\n",
    "\n",
    "\n",
    "def load_hdf5_data(image_files, depth_files):\n",
    "    \"\"\"\n",
    "    Load images and depth data from multiple HDF5 files.\n",
    "    \n",
    "    Parameters:\n",
    "        image_files (list): List of paths to the HDF5 files containing images.\n",
    "        depth_files (list): List of paths to the HDF5 files containing depth maps.\n",
    "    \n",
    "    Returns:\n",
    "        images (list): List of images.\n",
    "        depths (list): List of depth maps.\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    depths = []\n",
    "\n",
    "    for image_file, depth_file in zip(image_files, depth_files):\n",
    "        with h5py.File(image_file, 'r') as f:\n",
    "            images.append(np.array(f['dataset']))  # Adjust the key according to your HDF5 structure\n",
    "\n",
    "        with h5py.File(depth_file, 'r') as f:\n",
    "            depths.append(np.array(f['dataset']))  # Adjust the key according to your HDF5 structure\n",
    "\n",
    "    return images, depths\n",
    "\n",
    "\n",
    "\n",
    "def generate_point_clouds(images, depths, intrinsic_matrix, extrinsic_matrices): # extrinsic_matrices is not useful here, but will be used afterwards, having more images to put together\n",
    "    \"\"\"\n",
    "    Generate point clouds from images and depth maps and apply the camera extrinsic transformations.\n",
    "\n",
    "    Parameters:\n",
    "        images (list): List of images.\n",
    "        depths (list): List of depth maps.\n",
    "        intrinsic_matrix (np.ndarray): Intrinsic matrix of the camera.\n",
    "        extrinsic_matrices (list): List of camera extrinsic matrices.\n",
    "\n",
    "    Returns:\n",
    "        point_clouds (list): List of Open3D PointCloud objects.\n",
    "    \"\"\"\n",
    "    point_clouds = []\n",
    "\n",
    "    for image, depth in zip(images, depths):\n",
    "\n",
    "        # Get the image dimensions\n",
    "        h, w = depth.shape\n",
    "\n",
    "        # Create a mesh grid of pixel coordinates\n",
    "        u, v = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        u = u.flatten()\n",
    "        v = v.flatten()\n",
    "\n",
    "        # Get the corresponding depth values\n",
    "        z = depth.flatten()\n",
    "\n",
    "        # Filter out points with zero depth\n",
    "        valid = z > 0\n",
    "        u = u[valid]\n",
    "        v = v[valid]\n",
    "        z = z[valid]\n",
    "\n",
    "        # Convert pixel coordinates to normalized image coordinates; create 3D points in the camera coordinate system\n",
    "        x = (u - intrinsic_matrix[0, 2]) / intrinsic_matrix[0, 0] * z\n",
    "        y = (v - intrinsic_matrix[1, 2]) / intrinsic_matrix[1, 1] * z\n",
    "\n",
    "        # Stack the coordinates into a single array\n",
    "        points = np.vstack((x, y, z)).T\n",
    "\n",
    "\n",
    "        # The commented things will be useful when dealing with more than one image\n",
    "\n",
    "        # # Apply extrinsic transformation\n",
    "        # ones = np.ones((points.shape[0], 1))\n",
    "        # points_homogeneous = np.hstack((points, ones))\n",
    "        # points_transformed = (extrinsic_matrix @ points_homogeneous.T).T[:, :3]\n",
    "\n",
    "        # Create Open3D PointCloud object and add it to the list\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        #pcd.points = o3d.utility.Vector3dVector(points_transformed)\n",
    "        pcd.points = o3d.utility.Vector3dVector(points) # to be deleted when having more than one image\n",
    "        point_clouds.append(pcd)\n",
    "\n",
    "    return point_clouds\n",
    "\n",
    "\n",
    "\n",
    "def render_point_cloud(point_cloud):\n",
    "\n",
    "    pcd = point_cloud\n",
    "\n",
    "    # Create a visualization window\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "\n",
    "    # Add the point cloud to the visualization window\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    # Set the render options (optional)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.point_size = 2  # Adjust the size of the points\n",
    "\n",
    "    # Render the visualization\n",
    "    vis.run()\n",
    "\n",
    "    # Close the visualization window\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = '/local/home/gmarsich/Desktop/Thesis/0Code_playground/pointClouds/pointClouds_ChatGPT/metadata_camera_parameters.csv'\n",
    "base_path = '/local/home/gmarsich/data2TB/Hypersim/evermotion_dataset/scenes'\n",
    "scene = 'ai_007_008'  # name of the scene, with format ai_VVV_NNN\n",
    "cam_xx = 'cam_00'\n",
    "\n",
    "# Get list of image and depth HDF5 files\n",
    "image_files = sorted(glob.glob(os.path.join(base_path, scene, 'images', 'scene_' + cam_xx + '_final_hdf5', '*.color.hdf5')))\n",
    "position_files = sorted(glob.glob(os.path.join(base_path, scene, 'images', 'scene_' + cam_xx + '_geometry_hdf5', '*.depth_meters.hdf5')))\n",
    "\n",
    "# Ensure the number of image and depth files match\n",
    "if len(image_files) != len(position_files):\n",
    "    raise ValueError(\"The number of image files and depth files do not match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_width:  1024.0\n",
      "camera_physical_horizontal_shift\": 0.0\n",
      "img_height :  768.0\n",
      "camera_physical_lens_shift\": -0.0799999982118606\n",
      "Intrinsic Matrix for the scene:\n",
      "[[1.43810735e+03 0.00000000e+00 5.12000000e+02]\n",
      " [0.00000000e+00 1.43810735e+03 3.83920000e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "PointCloud with 786431 points.\n"
     ]
    }
   ],
   "source": [
    "# Get the intrinsic matrix for the scene\n",
    "intrinsics = get_intrinsics_with_tilt_shift_COMPLETE(path_metadata, scene)\n",
    "print(\"Intrinsic Matrix for the scene:\")\n",
    "print(intrinsics)\n",
    "\n",
    "# Load images and depth data\n",
    "image_files, position_files = load_hdf5_data(image_files, position_files)\n",
    "\n",
    "# Generate point clouds for each view\n",
    "point_clouds = generate_point_clouds(image_files, position_files, intrinsics, 1) # here extrinsics_matrices = 1 because it is not used, the value 1 is random\n",
    "\n",
    "cloud = point_clouds[0]\n",
    "print(cloud)\n",
    "\n",
    "render_point_cloud(point_clouds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOD - No scene_row[\"camera_physical_horizontal_shift\"] and scene_row[\"camera_physical_lens_shift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrinsics_from_metadata_NoHorNoLens(scene_row):\n",
    "    \"\"\"\n",
    "    Derive the intrinsic matrix from metadata.\n",
    "\n",
    "    Parameters:\n",
    "        scene_row (pd.Series): Metadata row for the scene.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The intrinsic matrix K.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the necessary parameters from the metadata\n",
    "    img_width = scene_row[\"settings_output_img_width\"]\n",
    "    img_height = scene_row[\"settings_output_img_height\"]\n",
    "    focal_length = scene_row[\"camera_physical_focal_length\"]\n",
    "    sensor_width = scene_row[\"camera_physical_film_width\"]\n",
    "\n",
    "    # Calculate fx and fy using the focal length and sensor width\n",
    "    fx = focal_length / sensor_width * img_width\n",
    "    fy = fx  # Assuming square pixels\n",
    "\n",
    "    # Calculate the principal point (cx, cy) considering horizontal and vertical shifts\n",
    "    cx = img_width / 2\n",
    "    print('img_width: ', img_width)\n",
    "    print('camera_physical_horizontal_shift:', scene_row[\"camera_physical_horizontal_shift\"])\n",
    "    cy = img_height / 2\n",
    "    print('img_height: ', img_height)\n",
    "    print('camera_physical_lens_shift:', scene_row[\"camera_physical_lens_shift\"])\n",
    "\n",
    "    # Construct the intrinsic matrix\n",
    "    intrinsic_matrix = np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    return intrinsic_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_intrinsics_with_tilt_shift_NoHorNoLens(path_metadata, scene_name):\n",
    "    \"\"\"\n",
    "    Load camera metadata from a CSV file and compute the intrinsic matrix considering tilt-shift parameters.\n",
    "\n",
    "    Parameters:\n",
    "        path_metadata (str): Path to the metadata CSV file.\n",
    "        scene_name (str): Name of the scene to analyze.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated intrinsic matrix K.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the metadata CSV file\n",
    "    df = pd.read_csv(path_metadata)\n",
    "    \n",
    "    # Filter the DataFrame to get the row corresponding to the specified scene\n",
    "    scene_row = df[df['scene_name'] == scene_name]\n",
    "    \n",
    "    if scene_row.empty:\n",
    "        raise ValueError(f\"Scene '{scene_name}' not found in the metadata.\")\n",
    "    \n",
    "    # Use the first matching row\n",
    "    scene_row = scene_row.iloc[0]\n",
    "    \n",
    "    # Get the intrinsic matrix from metadata\n",
    "    intrinsic_matrix = get_intrinsics_from_metadata_NoHorNoLens(scene_row)\n",
    "    \n",
    "    return intrinsic_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intrinsic matrix for the scene\n",
    "intrinsics = get_intrinsics_with_tilt_shift_NoHorNoLens(path_metadata, scene)\n",
    "print(\"Intrinsic Matrix for the scene:\")\n",
    "print(intrinsics)\n",
    "\n",
    "# Load images and depth data\n",
    "image_files, position_files = load_hdf5_data(image_files, position_files)\n",
    "\n",
    "# Generate point clouds for each view\n",
    "point_clouds = generate_point_clouds(image_files, position_files, intrinsics, 1) # here extrinsics_matrices = 1 because it is not used, the value 1 is random\n",
    "\n",
    "cloud = point_clouds[0]\n",
    "print(cloud)\n",
    "\n",
    "render_point_cloud(point_clouds[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisPlayground_pointClouds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
