{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment: thesisPlayground_pointClouds_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using depths\n",
    "### No indications on the color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints on how to deal with this problem from:\n",
    "- https://github.com/apple/ml-hypersim/issues/9 (source for the code for the conversion Euclidean distance in meters to the optical center of the camera -> distances wrt the plane of the camera)\n",
    "- https://github.com/apple/ml-hypersim/issues/10 (found where to get the conversion asset units <-> meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depths_oneImage(path_file, path_metadata_scene, intWidth = 1024, intHeight = 768, fltFocal = 886.81): # TODO: understand fltFocal\n",
    "\n",
    "    '''\n",
    "    Processes a dataset containing depth information of an image, and transforms the depth values to be relative to the plane of a camera.\n",
    "    This function reads the depth data from an HDF5 file, whose path is given, gets the depth values for each pixel, and applies a transformation\n",
    "    to obtain the depth values relative to the camera plane.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the HDF5 file containing the dataset with depth information, in meters.\n",
    "    - path_metadata_scene (str): The path to the csv file to get the conversion meters <-> asset units\n",
    "    - intWidth (int, optional): The width of the image in pixels. Default is 1024.\n",
    "    - intHeight (int, optional): The height of the image in pixels. Default is 768.\n",
    "    - fltFocal (float, optional): The focal length of the camera. Default is 886.81.\n",
    "\n",
    "    Returns:\n",
    "    - npyDepths (numpy.ndarray): A 2D numpy array of shape (intHeight, intWidth) containing the depth values for each pixel in the image,\n",
    "        relative to the camera plane. Measures in asset coordinates\n",
    "    '''\n",
    "\n",
    "    data_frame = pd.read_csv(path_metadata_scene)\n",
    "    meters_per_asset_unit = data_frame.loc[data_frame['parameter_name'] == 'meters_per_asset_unit', 'parameter_value'].values[0]\n",
    "\n",
    "\n",
    "    with h5py.File(path_file, 'r') as file:\n",
    "        # Access the dataset and read the data\n",
    "        dataset = file['dataset']\n",
    "        data = dataset[:]\n",
    "        \n",
    "        # Define the list of 2D points (row, column) to get the depth values for\n",
    "        points = [(y, x) for y in range(intHeight) for x in range(intWidth)]\n",
    "        \n",
    "        # Get the depth values for the specified points\n",
    "        npyDistances_array = [data[y, x] for y, x in points]\n",
    "        \n",
    "        # Apply transformation (Euclidean distance in meters to the optical center of the camera -> distances wrt the plane of the camera)\n",
    "        npyImageplaneX = np.linspace((-0.5 * intWidth) + 0.5, (0.5 * intWidth) - 0.5, intWidth).reshape(1, intWidth).repeat(intHeight, 0).astype(np.float32)[:, :, None]\n",
    "        npyImageplaneY = np.linspace((-0.5 * intHeight) + 0.5, (0.5 * intHeight) - 0.5, intHeight).reshape(intHeight, 1).repeat(intWidth, 1).astype(np.float32)[:, :, None]\n",
    "        npyImageplaneZ = np.full([intHeight, intWidth, 1], fltFocal, np.float32)\n",
    "        npyImageplane = np.concatenate([npyImageplaneX, npyImageplaneY, npyImageplaneZ], 2)\n",
    "\n",
    "        npyDistances = np.array(npyDistances_array)\n",
    "        npyDistances = npyDistances.reshape(768, 1024)\n",
    "\n",
    "        npyDepths_meters = npyDistances / np.linalg.norm(npyImageplane, 2, 2) * fltFocal\n",
    "\n",
    "        # Conversion meters -> asset units\n",
    "        npyDepths_assetCoo = npyDepths_meters / meters_per_asset_unit\n",
    "\n",
    "        return npyDepths_assetCoo\n",
    "\n",
    "\n",
    "def get_depths(path_list, path_metadata_scene):\n",
    "    '''\n",
    "    Processes a list of paths to HDF5 files containing depth information.\n",
    "    It iterates through each file path, applies the `get_depths_oneImage` function to obtain the depth values\n",
    "    relative to the camera plane for each image, and collects these depth representations into a list.\n",
    "\n",
    "    Parameters:\n",
    "    - path_list (list of str): A list of file paths to HDF5 files containing depth information.\n",
    "    - path_metadata_scene (str): The path to the csv file to get the conversion meters <-> asset units.\n",
    "\n",
    "    Returns:\n",
    "    - depth_files (list): A list containing representations of depth information for each image in `path_list`, and each element represents an image.\n",
    "        Measures in asset units\n",
    "    '''\n",
    "    \n",
    "    depth_files = []\n",
    "    for i in range(len(path_list)):\n",
    "        depth_files.append(get_depths_oneImage(path_list[i], path_metadata_scene))\n",
    "\n",
    "    return depth_files\n",
    "\n",
    "\n",
    "\n",
    "def extract_frame_ids(paths_filenames):\n",
    "    \"\"\"\n",
    "    Extracts frame IDs from a list of filenames.\n",
    "    \n",
    "    Parameters:\n",
    "        paths_filenames (list): List of file paths. Last info of a path is the filename.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of extracted frame IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    frame_ids = []\n",
    "    for path_filename in paths_filenames:\n",
    "        filename = path_filename.split('/')[-1]  # Extracting just the filename from the path\n",
    "        frame_id_str = filename.split('.')[1]\n",
    "        frame_id = int(frame_id_str)\n",
    "        frame_ids.append(frame_id)\n",
    "\n",
    "    return frame_ids\n",
    "\n",
    "\n",
    "def get_extrinsics_oneImage(path_positions, path_orientations, frame_id):\n",
    "    \"\"\"\n",
    "    Load camera position and orientation from a HDF5 file and compute the extrinsic matrix for that frame.\n",
    "\n",
    "    Parameters:\n",
    "        path_positions (str): Path to the camera positions HDF5 file. The file contains the data in asset units, not in meters\n",
    "        path_orientations (str): Path to the camera orientations HDF5 file. See https://github.com/apple/ml-hypersim/tree/main?tab=readme-ov-file#camera-trajectories\n",
    "            for more information\n",
    "        frame_id (int): Frame ID to extract the extrinsics for.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The extrinsic matrix [R|t], that is 3x4.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load camera position\n",
    "    with h5py.File(path_positions, \"r\") as f:\n",
    "        camera_positions = f[\"dataset\"][:]\n",
    "    \n",
    "    # Load camera orientation\n",
    "    with h5py.File(path_orientations, \"r\") as f:\n",
    "        camera_orientations = f[\"dataset\"][:]\n",
    "    \n",
    "    # Get position and rotation matrix for the specified frame\n",
    "    camera_position_world = camera_positions[frame_id]\n",
    "    R_world_from_cam = camera_orientations[frame_id]\n",
    "\n",
    "    # Construct the extrinsic matrix [R|t]\n",
    "    extrinsic_matrix = np.hstack((R_world_from_cam, camera_position_world.reshape(3, 1)))\n",
    "    \n",
    "    return extrinsic_matrix\n",
    "\n",
    "\n",
    "def get_extrinsics(path_positions, path_orientations, frames_ids):\n",
    "    \"\"\"\n",
    "    Load camera position and orientation from HDF5 files and compute the extrinsic matrix for each frame.\n",
    "\n",
    "    Parameters:\n",
    "        path_positions (str): Path to the camera positions HDF5 file. The file contains the data in asset units, not in meters\n",
    "        path_orientations (str): Path to the camera orientations HDF5 file. See https://github.com/apple/ml-hypersim/tree/main?tab=readme-ov-file#camera-trajectories\n",
    "            for more information\n",
    "        frames_id (list of int): Frames ID to extract the extrinsics for.\n",
    "\n",
    "    Returns:\n",
    "        extrinsics (list of numpy.ndarray): A list containing the extrinsic matrix [R|t] for each frame specified in `frames_id`.\n",
    "            Each extrinsic matrix is a 3x4 transformation matrix representing the camera's position and orientation in the world coordinate system.\n",
    "    \"\"\"\n",
    "    \n",
    "    extrinsics = []\n",
    "    for i in range(len(frames_ids)):\n",
    "        extrinsics.append(get_extrinsics_oneImage(path_positions, path_orientations, frames_ids[i]))\n",
    "\n",
    "    return extrinsics\n",
    "\n",
    "\n",
    "\n",
    "def get_intrinsics_with_tiltShift(path_metadata, scene_name):\n",
    "    \"\"\"\n",
    "    This function calculates the intrinsic matrix for a camera considering given tilt and shift effects, using metadata from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        path_metadata (str): Path to the metadata CSV file containing camera settings.\n",
    "        scene_name (str): Name of the scene for which the intrinsic matrix is computed.\n",
    "\n",
    "    Returns:\n",
    "        intrinsic_matrix (numpy.ndarray): 3x3 array representing the camera intrinsic matrix. \n",
    "    \"\"\"\n",
    "\n",
    "    # Load the metadata CSV file\n",
    "    df = pd.read_csv(path_metadata)\n",
    "    \n",
    "    # Filter the DataFrame to get the row corresponding to the specified scene\n",
    "    scene_row = df[df['scene_name'] == scene_name]\n",
    "    \n",
    "    if scene_row.empty:\n",
    "        raise ValueError(f\"Scene '{scene_name}' not found in the metadata.\")\n",
    "    \n",
    "    # Use the first matching row\n",
    "    scene_row = scene_row.iloc[0]\n",
    "\n",
    "    # Extract the necessary parameters from the metadata\n",
    "    img_width = scene_row[\"settings_output_img_width\"]\n",
    "    img_height = scene_row[\"settings_output_img_height\"]\n",
    "    focal_length = scene_row[\"camera_physical_focal_length\"]\n",
    "    sensor_width = scene_row[\"camera_physical_film_width\"]\n",
    "\n",
    "    # Calculate fx and fy using the focal length and sensor width\n",
    "    fx = focal_length / sensor_width * img_width\n",
    "    fy = fx  # Assuming square pixels\n",
    "\n",
    "    # Calculate the principal point (cx, cy) considering horizontal and vertical shifts\n",
    "    cx = img_width / 2 + scene_row[\"camera_physical_horizontal_shift\"]\n",
    "    cy = img_height / 2 + scene_row[\"camera_physical_lens_shift\"]\n",
    "\n",
    "    # Construct the intrinsic matrix\n",
    "    intrinsic_matrix = np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    return intrinsic_matrix\n",
    "\n",
    "\n",
    "\n",
    "def generate_point_clouds(depth_images, intrinsic_matrix, extrinsic_matrices):\n",
    "    \"\"\"\n",
    "    Generates point clouds from a list of depth images using the camera's intrinsic and extrinsic matrices.\n",
    "\n",
    "    Parameters:\n",
    "        depth_images (list of numpy.ndarray): A list of 2D numpy arrays representing the depth values for each pixel in each image.\n",
    "        intrinsic_matrix (numpy.ndarray): 3x3 array representing the camera intrinsic matrix.\n",
    "        extrinsic_matrices (list of numpy.ndarray): A list of 3x4 arrays representing the camera extrinsic matrices for each image.\n",
    "\n",
    "    Returns:\n",
    "        list of numpy.ndarray: A list containing numpy arrays containing the 3D coordinates of the point clouds for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store point clouds\n",
    "    point_clouds = []\n",
    "\n",
    "    # Loop through each depth image and its corresponding extrinsic matrix\n",
    "    for depth_image, extrinsic_matrix in zip(depth_images, extrinsic_matrices):\n",
    "        print(\"Doing one image out of \", len(depth_images))\n",
    "\n",
    "        # Get image width and height\n",
    "        height, width = depth_image.shape\n",
    "        \n",
    "        # Initialize an empty array to store 3D coordinates\n",
    "        point_cloud = []\n",
    "        \n",
    "        # Loop through each pixel in the depth image\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                # Get the depth value for the current pixel\n",
    "                depth = depth_image[y, x]\n",
    "                \n",
    "                # Skip if depth is invalid (e.g., zero)\n",
    "                if depth == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the 3D coordinates using the depth value, intrinsic matrix, and extrinsic matrix\n",
    "                homogeneous_pixel = np.array([x, y, 1])\n",
    "                camera_coordinates = np.linalg.inv(intrinsic_matrix) @ homogeneous_pixel * depth #TODO you may use a different function\n",
    "                world_coordinates = extrinsic_matrix @ np.append(camera_coordinates, 1)\n",
    "                \n",
    "                # Append the 3D coordinates to the point cloud\n",
    "                point_cloud.append(world_coordinates)\n",
    "        \n",
    "        # Convert the list of coordinates to a numpy array and append to the list of point clouds\n",
    "        point_cloud = np.array(point_cloud)\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "\n",
    "        point_clouds.append(pcd)\n",
    "        \n",
    "    return point_clouds\n",
    "\n",
    "\n",
    "def merge_point_clouds(point_clouds):\n",
    "    \"\"\"\n",
    "    Merges multiple Open3D PointCloud objects into a single PointCloud object.\n",
    "\n",
    "    Parameters:\n",
    "        point_clouds (list of o3d.geometry.PointCloud): A list of Open3D PointCloud objects.\n",
    "\n",
    "    Returns:\n",
    "        o3d.geometry.PointCloud: A single Open3D PointCloud object containing the merged 3D coordinates of all point clouds.\n",
    "    \"\"\"\n",
    "    # Create an empty Open3D PointCloud object to store the merged point cloud\n",
    "    merged_point_cloud = o3d.geometry.PointCloud()\n",
    "    \n",
    "    # Concatenate all the individual point clouds into a single point cloud\n",
    "    for pcd in point_clouds:\n",
    "        merged_point_cloud += pcd\n",
    "    \n",
    "    return merged_point_cloud\n",
    "\n",
    "\n",
    "def render_point_cloud(point_cloud):\n",
    "    \"\"\"\n",
    "    Renders a 3D point cloud using Open3D.\n",
    "\n",
    "    Parameters:\n",
    "        point_cloud (o3d.geometry.PointCloud): An Open3D PointCloud object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a visualization window\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "\n",
    "    # Add the point cloud to the visualization window\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(point_cloud)\n",
    "\n",
    "    # Set the render options (optional)\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.point_size = 2  # Adjust the size of the points\n",
    "\n",
    "    # Render the visualization\n",
    "    vis.run()\n",
    "\n",
    "    # Close the visualization window\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrinsics Matrix for the first image of the scene:\n",
      "[[ 9.99448895e-01  2.52561085e-03 -3.30993086e-02  7.81024837e+00]\n",
      " [-3.31955254e-02  7.60409459e-02 -9.96551991e-01 -2.44333206e+02]\n",
      " [ 0.00000000e+00  9.97101486e-01  7.60828778e-02  4.44449768e+01]]\n",
      "Intrinsic Matrix for the scene:\n",
      "[[1.43810735e+03 0.00000000e+00 5.12000000e+02]\n",
      " [0.00000000e+00 1.43810735e+03 3.83920000e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "base_path = '/local/home/gmarsich/data2TB/Hypersim/evermotion_dataset/scenes'\n",
    "scene = 'ai_007_008'  # name of the scene, with format ai_VVV_NNN\n",
    "cam_xx = 'cam_00'\n",
    "\n",
    "path_metadata_camera_parameters = '/local/home/gmarsich/Desktop/Thesis/0Code_playground/pointClouds/pointClouds_ChatGPT/withDepths/metadata_camera_parameters.csv'\n",
    "path_metadata_scene = os.path.join(base_path, scene, '_detail', 'metadata_scene.csv')\n",
    "path_orientations = os.path.join(base_path, scene, '_detail', cam_xx, 'camera_keyframe_orientations.hdf5')\n",
    "path_positions = os.path.join(base_path, scene, '_detail', cam_xx, 'camera_keyframe_positions.hdf5')\n",
    "\n",
    "# Get list of depth HDF5 files\n",
    "depthEuclidean_paths = sorted(glob.glob(os.path.join(base_path, scene, 'images', 'scene_' + cam_xx + '_geometry_hdf5', '*.depth_meters.hdf5')))\n",
    "\n",
    "# Get depth information for each image\n",
    "depth_files = get_depths(depthEuclidean_paths, path_metadata_scene)\n",
    "\n",
    "# Get extrinsics matrices, one matrix per image\n",
    "frames_ids = extract_frame_ids(depthEuclidean_paths)\n",
    "\n",
    "extrinsics = get_extrinsics(path_positions, path_orientations, frames_ids)\n",
    "print(\"Extrinsics Matrix for the first image of the scene:\")\n",
    "print(extrinsics[0])\n",
    "\n",
    "# Get the intrinsic matrix for the scene\n",
    "intrinsics = get_intrinsics_with_tiltShift(path_metadata_camera_parameters, scene)\n",
    "print(\"Intrinsic Matrix for the scene:\")\n",
    "print(intrinsics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing one image out of  3\n",
      "Doing one image out of  3\n",
      "Doing one image out of  3\n"
     ]
    }
   ],
   "source": [
    "# Generate the point clouds\n",
    "point_clouds = generate_point_clouds(depth_files, intrinsics, extrinsics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_point_cloud = merge_point_clouds(point_clouds)\n",
    "render_point_cloud(merged_point_cloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisPlayground_pointClouds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
